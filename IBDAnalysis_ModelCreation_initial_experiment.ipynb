{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ed922b9",
      "metadata": {
        "id": "9ed922b9"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a14JrlB8Hk8S"
      },
      "id": "a14JrlB8Hk8S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6ea6d44b",
      "metadata": {
        "id": "6ea6d44b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6efd062-3e85-475d-a4ea-8f8678996058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcb7VY6aPVOy",
        "outputId": "3af3188e-a411-4f19-bbd5-c8411f3443eb"
      },
      "id": "Qcb7VY6aPVOy",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "afa89d72",
      "metadata": {
        "id": "afa89d72"
      },
      "outputs": [],
      "source": [
        "#Loading the final cleaned datasets\n",
        "\n",
        "merged_file_df = pd.read_csv('drive/MyDrive/college/northeastern-university/ExperientialProject/Merged_File_11.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f41fdbe",
      "metadata": {
        "id": "3f41fdbe"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c29ff0e3",
      "metadata": {
        "id": "c29ff0e3",
        "outputId": "4eb0bb5a-9471-4b3c-d2f9-d450a5a06a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object\n"
          ]
        }
      ],
      "source": [
        "print(merged_file_df['Exam_Notes'].dtype)\n",
        "merged_file_df['Image_Info'] = merged_file_df['Image_Info'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ed1f0e1f",
      "metadata": {
        "id": "ed1f0e1f"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "merged_file_df['Exam_Notes_tokens'] = merged_file_df['Exam_Notes'].apply(lambda x: word_tokenize(x))\n",
        "merged_file_df['Image_Info_tokens'] = merged_file_df['Image_Info'].apply(lambda x: word_tokenize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b18643",
      "metadata": {
        "id": "86b18643"
      },
      "outputs": [],
      "source": [
        "merged_file_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA with TFIDF"
      ],
      "metadata": {
        "id": "Nk-kXix56Lv1"
      },
      "id": "Nk-kXix56Lv1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming you have your dataset loaded into a DataFrame called 'data'\n",
        "# with columns 'Exam_ID' and 'Exam_Notes'\n",
        "\n",
        "# Filter the dataset for the 'Active' class\n",
        "active_data = merged_file_df[merged_file_df['Disease_Severity'] == 'Active']\n",
        "\n",
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(active_data['Exam_Notes'])\n",
        "\n",
        "# Apply PCA to reduce the dimensionality\n",
        "pca = PCA(n_components=5)\n",
        "pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "# Get the list of important words (features) based on PCA components\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "component_weights = pca.components_\n",
        "\n",
        "# Find the most important words for the 'Active' class\n",
        "top_words = []\n",
        "for component in component_weights:\n",
        "    top_word_indices = component.argsort()[-5:]  # Select top 5 words per component\n",
        "    top_words.extend([feature_names[idx] for idx in top_word_indices])\n",
        "\n",
        "# Remove duplicates\n",
        "top_words = list(set(top_words))\n",
        "\n",
        "# Print the important words\n",
        "print(\"Important words for the 'Active' class:\")\n",
        "for word in top_words:\n",
        "    print(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZgbYRmw6LbD",
        "outputId": "1bab7a7c-ea64-4949-abba-b0b940de11a8"
      },
      "id": "SZgbYRmw6LbD",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important words for the 'Active' class:\n",
            "present\n",
            "mesalamine\n",
            "office\n",
            "discharge\n",
            "activities\n",
            "discussed\n",
            "monitored\n",
            "continue\n",
            "per\n",
            "scheduled\n",
            "tomorrow\n",
            "care\n",
            "propofol\n",
            "entocort\n",
            "return\n",
            "physician\n",
            "olympus\n",
            "budesonide\n",
            "gi\n",
            "referring\n",
            "found\n",
            "op\n",
            "clinic\n",
            "home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_words + ['ileitis', 'ulcer', 'ulceration', 'erosion', 'aphtha', 'aphthae', 'aphthous']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T1eZNow9Zdy",
        "outputId": "bcd7e795-7639-4240-b4d8-2b105fc37d1d"
      },
      "id": "5T1eZNow9Zdy",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['present',\n",
              " 'mesalamine',\n",
              " 'office',\n",
              " 'discharge',\n",
              " 'activities',\n",
              " 'discussed',\n",
              " 'monitored',\n",
              " 'continue',\n",
              " 'per',\n",
              " 'scheduled',\n",
              " 'tomorrow',\n",
              " 'care',\n",
              " 'propofol',\n",
              " 'entocort',\n",
              " 'return',\n",
              " 'physician',\n",
              " 'olympus',\n",
              " 'budesonide',\n",
              " 'gi',\n",
              " 'referring',\n",
              " 'found',\n",
              " 'op',\n",
              " 'clinic',\n",
              " 'home',\n",
              " 'ileitis',\n",
              " 'ulcer',\n",
              " 'ulceration',\n",
              " 'erosion',\n",
              " 'aphtha',\n",
              " 'aphthae',\n",
              " 'aphthous']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(vocabulary=top_words)  # Pass the important words as the vocabulary\n",
        "\n",
        "# Vectorize the Exam Notes data for the 'Active' class using the important words\n",
        "X = vectorizer.fit_transform(merged_file_df['Exam_Notes'])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, merged_file_df['Disease_Severity'], test_size=0.3, random_state=40)\n",
        "\n",
        "# Train a machine learning model (e.g., Logistic Regression)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-gbMx6g7Qv8",
        "outputId": "ca69be50-7a16-4848-bcd8-ee7d38bc02f6"
      },
      "id": "c-gbMx6g7Qv8",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Active       0.56      0.43      0.49       104\n",
            "    Inactive       0.58      0.70      0.63       115\n",
            "\n",
            "    accuracy                           0.57       219\n",
            "   macro avg       0.57      0.56      0.56       219\n",
            "weighted avg       0.57      0.57      0.56       219\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(vocabulary=set(top_words))  # Convert the top_words list to a set\n",
        "\n",
        "# Vectorize the Exam Notes data using the important words\n",
        "X = vectorizer.fit_transform(merged_file_df['Exam_Notes'])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, merged_file_df['Disease_Severity'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 400],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_model = RandomForestClassifier(**best_params)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVyZTcmN-nxs",
        "outputId": "b4c0dee7-4e65-4b8f-fd32-9aab4f35d448"
      },
      "id": "BVyZTcmN-nxs",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Active       0.58      0.66      0.62       102\n",
            "    Inactive       0.66      0.59      0.62       117\n",
            "\n",
            "    accuracy                           0.62       219\n",
            "   macro avg       0.62      0.62      0.62       219\n",
            "weighted avg       0.63      0.62      0.62       219\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN model"
      ],
      "metadata": {
        "id": "iV0V1rTf-mYD"
      },
      "id": "iV0V1rTf-mYD"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Preprocess the data\n",
        "X = merged_file_df['Exam_Notes'].values\n",
        "y = merged_file_df['Disease_Severity'].values\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(vocabulary=top_words)  # Pass the important words as the vocabulary\n",
        "\n",
        "# Vectorize the Exam Notes data for the 'Active' class using the important words\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Convert the dense matrix to a SparseTensor\n",
        "X_train_sparse = tf.sparse.SparseTensor(\n",
        "    indices=np.vstack(X_train_vectorized.nonzero()).T,\n",
        "    values=X_train_vectorized.data,\n",
        "    dense_shape=X_train_vectorized.shape\n",
        ")\n",
        "\n",
        "# Reorder the sparse matrix indices\n",
        "X_train_vectorized_reordered = tf.sparse.reorder(X_train_sparse)\n",
        "\n",
        "# Convert SparseTensor to dense matrix\n",
        "X_train_dense = tf.sparse.to_dense(X_train_vectorized_reordered).numpy()\n",
        "\n",
        "all_notes = merged_file_df['Exam_Notes'].values\n",
        "vectorizer = TfidfVectorizer(vocabulary=top_words)\n",
        "vectorizer.fit(all_notes)\n",
        "vocab_size = len(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Determine the maximum sequence length\n",
        "max_seq_length = X_train_vectorized.shape[1]\n",
        "\n",
        "# Define the RNN model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_seq_length))\n",
        "model.add(LSTM(units=64, dropout=0.1))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_dense, y_train, epochs=100, batch_size=24, validation_split=0.2)\n",
        "\n",
        "# Convert the test data to a SparseTensor\n",
        "X_test_sparse = tf.sparse.SparseTensor(\n",
        "    indices=np.vstack(X_test_vectorized.nonzero()).T,\n",
        "    values=X_test_vectorized.data,\n",
        "    dense_shape=X_test_vectorized.shape\n",
        ")\n",
        "\n",
        "# Reorder the sparse matrix indices\n",
        "X_test_vectorized_reordered = tf.sparse.reorder(X_test_sparse)\n",
        "\n",
        "# Convert SparseTensor to dense matrix\n",
        "X_test_dense = tf.sparse.to_dense(X_test_vectorized_reordered).numpy()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_dense, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9agxTZ05AsWm",
        "outputId": "ec4769f9-c91f-4d97-f4ca-c37af237281b"
      },
      "id": "9agxTZ05AsWm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.7818 - accuracy: 0.4774"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7fdf01c711b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 103, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15/15 [==============================] - 80s 5s/step - loss: 0.7818 - accuracy: 0.4774 - val_loss: 0.7545 - val_accuracy: 0.4957\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 0.7596 - accuracy: 0.4774 - val_loss: 0.7404 - val_accuracy: 0.4957\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 1s 77ms/step - loss: 0.7463 - accuracy: 0.4774 - val_loss: 0.7304 - val_accuracy: 0.4957\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 1s 63ms/step - loss: 0.7368 - accuracy: 0.4774 - val_loss: 0.7233 - val_accuracy: 0.4957\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 1s 60ms/step - loss: 0.7286 - accuracy: 0.4774 - val_loss: 0.7181 - val_accuracy: 0.4957\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.7244 - accuracy: 0.4774 - val_loss: 0.7138 - val_accuracy: 0.4957\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 1s 58ms/step - loss: 0.7190 - accuracy: 0.4774 - val_loss: 0.7103 - val_accuracy: 0.4957\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 0.7153 - accuracy: 0.4774 - val_loss: 0.7074 - val_accuracy: 0.4957\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 0.7119 - accuracy: 0.4774 - val_loss: 0.7050 - val_accuracy: 0.4957\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.7090 - accuracy: 0.4774 - val_loss: 0.7029 - val_accuracy: 0.4957\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.7072 - accuracy: 0.4774 - val_loss: 0.7013 - val_accuracy: 0.4957\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.7053 - accuracy: 0.4774 - val_loss: 0.7000 - val_accuracy: 0.4957\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.7031 - accuracy: 0.4774 - val_loss: 0.6988 - val_accuracy: 0.4957\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 1s 48ms/step - loss: 0.7012 - accuracy: 0.4774 - val_loss: 0.6979 - val_accuracy: 0.4957\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 0.7004 - accuracy: 0.4774 - val_loss: 0.6971 - val_accuracy: 0.4957\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 1s 48ms/step - loss: 0.7001 - accuracy: 0.4774 - val_loss: 0.6965 - val_accuracy: 0.4957\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.6987 - accuracy: 0.4774 - val_loss: 0.6958 - val_accuracy: 0.4957\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 1s 79ms/step - loss: 0.6972 - accuracy: 0.4774 - val_loss: 0.6954 - val_accuracy: 0.4957\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.6972 - accuracy: 0.4774 - val_loss: 0.6949 - val_accuracy: 0.4957\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.6959 - accuracy: 0.4774 - val_loss: 0.6945 - val_accuracy: 0.4957\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 0.6954 - accuracy: 0.4774 - val_loss: 0.6943 - val_accuracy: 0.4957\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.6951 - accuracy: 0.4774 - val_loss: 0.6941 - val_accuracy: 0.4957\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 0.6954 - accuracy: 0.4774 - val_loss: 0.6939 - val_accuracy: 0.4957\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.6947 - accuracy: 0.4774 - val_loss: 0.6938 - val_accuracy: 0.4957\n",
            "Epoch 25/100\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 0.6937 - accuracy: 0.4773"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a4422f",
      "metadata": {
        "id": "02a4422f"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0873a400",
      "metadata": {
        "id": "0873a400"
      },
      "outputs": [],
      "source": [
        "def BagOfWordsVectorization(columns,newColumn):\n",
        "    # Get the tokenized text from the DataFrame\n",
        "    tokenized_text = merged_file_df[columns]\n",
        "\n",
        "    # Convert tokenized text back to strings\n",
        "    preprocessed_text = [' '.join(tokens) for tokens in tokenized_text]\n",
        "\n",
        "    # Create an instance of CountVectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "    # Fit the vectorizer on the preprocessed text and transform it\n",
        "    vectorized_data = vectorizer.fit_transform(preprocessed_text).toarray()\n",
        "\n",
        "    # Create a new column in the DataFrame with the vectorized data\n",
        "    merged_file_df[newColumn] = pd.Series(vectorized_data.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9b015e",
      "metadata": {
        "id": "bd9b015e"
      },
      "outputs": [],
      "source": [
        "BagOfWordsVectorization('Exam_Notes_tokens','Exam_Notes_vectorized')\n",
        "BagOfWordsVectorization('Image_Info_tokens','ImageInfo_vectorized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9a170b",
      "metadata": {
        "id": "2b9a170b"
      },
      "outputs": [],
      "source": [
        "merged_file_df = merged_file_df.drop(['Exam_ID','Exam_Notes','Exam_Notes_tokens','Image_Info_tokens','Image_Info'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9d8267",
      "metadata": {
        "id": "8a9d8267"
      },
      "source": [
        "## Encoding categeorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ffd3b9",
      "metadata": {
        "id": "e3ffd3b9"
      },
      "outputs": [],
      "source": [
        "merged_file_df[\"Disease_Severity\"] = merged_file_df[\"Disease_Severity\"].apply(lambda x: 1 if x == \"Active\" else 0)\n",
        "merged_file_df[\"Ulcer\"] = merged_file_df[\"Ulcer\"].apply(lambda x: 1 if x == \"Yes\" else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f1a5d1",
      "metadata": {
        "id": "c9f1a5d1",
        "outputId": "e5215e51-2d40-4129-ca68-928ac0a1f67a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disease_Severity</th>\n",
              "      <th>Ulcer</th>\n",
              "      <th>Exam_Notes_vectorized</th>\n",
              "      <th>ImageInfo_vectorized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Disease_Severity  Ulcer                              Exam_Notes_vectorized  \\\n",
              "0                 1      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1                 0      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2                 1      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3                 0      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4                 1      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                ImageInfo_vectorized  \n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_file_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbb641ce",
      "metadata": {
        "id": "bbb641ce"
      },
      "source": [
        "## Splitting of Training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f44d50",
      "metadata": {
        "id": "34f44d50"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(merged_file_df.drop('Disease_Severity', axis=1), merged_file_df['Disease_Severity'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cdd946f",
      "metadata": {
        "id": "6cdd946f"
      },
      "source": [
        "## Model Building SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c4ee30",
      "metadata": {
        "id": "22c4ee30"
      },
      "outputs": [],
      "source": [
        "# Convert the word vectors to a numeric format\n",
        "X_train_exam = pd.DataFrame(X_train['Exam_Notes_vectorized'].apply(pd.Series))\n",
        "X_train_image = pd.DataFrame(X_train['ImageInfo_vectorized'].apply(pd.Series))\n",
        "X_train = pd.concat([X_train_exam, X_train_image], axis=1)\n",
        "\n",
        "X_test_exam = pd.DataFrame(X_test['Exam_Notes_vectorized'].apply(pd.Series))\n",
        "X_test_image = pd.DataFrame(X_test['ImageInfo_vectorized'].apply(pd.Series))\n",
        "X_test = pd.concat([X_test_exam, X_test_image], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666df5ca",
      "metadata": {
        "id": "666df5ca",
        "outputId": "9c61847f-0b7e-46a6-dde1-51c7899044e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.7243150684931506\n",
            "Testing Accuracy: 0.6438356164383562\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.62      0.62        69\n",
            "           1       0.66      0.66      0.66        77\n",
            "\n",
            "    accuracy                           0.64       146\n",
            "   macro avg       0.64      0.64      0.64       146\n",
            "weighted avg       0.64      0.64      0.64       146\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create an SVM classifier object\n",
        "svm_model = SVC(C=1, kernel='poly',degree=1)\n",
        "\n",
        "# Fit the SVM classifier on the training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_train = svm_model.predict(X_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the SVM model\n",
        "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "print(\"Training accuracy:\", accuracy_train)\n",
        "\n",
        "# Evaluate the performance of the SVM model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Testing Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df3bea86",
      "metadata": {
        "id": "df3bea86"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}